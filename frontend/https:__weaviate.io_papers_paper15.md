# https:\_\_weaviate.io_papers_paper15

Persuasive Adversarial Prompting to Jailbreak LLMs with 92% Success Rate. More advanced models like GPT-4 are more vulnerable to persuasive adversarial prompts (PAPs) Test these PAPs to perform attacks covering 14 different risk categories (such as economic harm, etc.)
